# cyber-security-deception-database-service

In the realm of cybersecurity, experts propose defensive deception as a strategy to exploit the information imbalance favored by attackers. This involves creating fake services to divert attackers from critical assets. The project aims to develop a Docker image hosting a simulated Relational Database with auto-generation capabilities for complex schemas and data. The image allows importing existing schemas, configuring client connections, and is OCI-compatible. The container functions out of the box, and any necessary configurations are provided during image generation. The goal is to lure attackers away from real assets by engaging them with deceptive elements.

This software allows to generate a complex Postgres database structure, customizing the roles, users, permissions, databases and schemas. This can be done by the user via yaml configuration files. The syntethic data (tables columns and rows) that will populate the database tables inside the schemas will be generated by LLaMa2.


## Prerequisites
* Docker
* LLaMa-2 model released by Meta. The llama.cpp project will be used as the inference engine.
* llama-cpp-python: to interact with llama.cpp via python.

The LLaMa-2 model's parameters can be downloaded from: https://ai.meta.com/resources/models-and-libraries/llama-downloads
There're several versions available, for this project the 7 billion *chat* version will be used.

The main goal of [llama.cpp](https://github.com/ggerganov/llama.cpp) is to run the LLaMA model in plain C/C++ implementation without dependencies, supporting lots of optmization techniques such as quantization.

In order to get started with llama.cpp:
1. Clone the repo and make a virtual environment:
```
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
python3 -m venv llamacpp_env
source activate llamacpp_env/bin/activate
pip install -r requirements.txt
```
2. The LLaMa model's weights must be converted with: 
```
python3 convert.py ../llm-models/llama2/llama-2-7b-chat --vocab-dir ./llm-models/llama2/
mkdir -p ./models/7B && cp ../llm-models/llama2/llama-2-7b-chat/ggml-model-f16.gguf ./models/7B
```
3. An optional quantization conversion can be applied to reduce the model size: 
```
./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0
```
4. The inference engine can be started in interactive mode with:
```
./main -m ./models/7B/ggml-model-f16.gguf --color --ctx_size 2048 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 8
```

The [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) library allows the interaction with llama.cpp via a simple Python API.

CPU build:
```
pip install llama-cpp-python
```
Cuda build:
```
export LLAMA_CUBLAS=1
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir --verbose
```
Once the library is installed, the inference can be run as follow:
```
llm = Llama(
    model_path="llama.cpp/models/7B/ggml-model-f16.gguf",
    chat_format="llama-2"
)
r = llm.create_chat_completion(
      messages = [
          {"role": "system", "content": "You are a skilled SQL programmer and AI assistent that never fails to answer the User's requests immediately and with precision"},
          {
              "role": "user",
              "content": "Generate SQL code to create a relational database schema for a marketing company composed by at least 5 tables."
          }
      ]
)
```


## How does it work 

### Step 0: write the yaml 
Three configuration files (`postgres.yaml`, `user_and_roles.yaml` and `dbs.yaml`) describe the system in terms of Postgres configuration, databases and users with their authentication and authorization policies driven by roles.

The configuration in `postgres.yaml` allows to define the port and the ip addresses used by Postgres.

The configuration in `user_and_roles.yaml` allows to define a custom database structure under the following assumption: there are super-roles (called J*) and sub-roles. The super-roles have read-access to the databases created by the sub-roles, while sub-roles permissions are confined to their own databases. For example: 
```
roles:
  j1:
    - promotion
    - employees
    - welfare
    - general_affairs
```
In the same configuration files, are listed all the users of the databases along with their name, role and password. Users will have the *LOGIN* and *PASSWORD* options and will inherit roles grants. For example: 
```
users:
  - name: j1_chief
    role: j1
    password: user_chief_j1

  - name: j1_user_1
    role: promotion
    password: user_1 
```

The `dbs.yaml` configuration file lists all the databases along with their schemas, authentication and authorization policies. The *owner* key represents the role with which the database will be created. CRUD authorizations are given to the roles. The authentication is specified by the *connections* key to allow connection (unix-socket or tcp/ip) to the database. The *auth* key specify the encryption. Finally, the *schema* key lists the schemas that will be generated inside the db. The *subject* and *number_of_tables* keys are included in the prompt that will command the LLM to generate the tables.
```
dbs:
  - name: pe_1
    owner: j1
    grant_read_access_to:
      - promotion
      - employees
    grant_write_access_to:
      - promotion
      - employees
    grant_update_access_to:
      - promotion
      - employees
    grant_delete_access_to:
      - promotion
      - employees
    schemas:
      - name: hr
        subject: "human resources"
        number_of_tables: 3
      - name: pr
        subject: "promotions"
        number_of_tables: 3
    connections:
      - local
      - hostnossl
    auth: md5
```
### Step 1: create the Postgres configuration files
The script `bootstrap.py` starts from the user-defined yaml configuration files `postgres.yaml`, `users_and_roles.yaml` and `dbs.yaml` (described later), and it generates Postgres configuration files `postgres.conf`, `pg_hba.conf` and psql scripts `0_init-roles.sh`, `1_init-users.sh`, `2_init-db-*.sh`, `3_init-schemas-*.sh`, `4_init-authorization-*.sh` that will be executed by Postgres during the Docker container creation process to configure the Postgres service, create roles, users,  databases, schemas (Postgres namespaces) and grant authorizations.

```bash
python bootstrap.py -p ./image_configuration/user_configurations/postgres.yaml -d ./image_configuration/user_configurations/dbs.yaml -u ./image_configuration/user_configurations/users_and_roles.yaml
```

### Step 2: 
Start a Postgres container. It will be used by the following steps. 
```
docker run --name sandbox-postgres -p 9999:5432 -e POSTGRES_PASSWORD=postgres -d postgres:16.1
```

### Step 3: create the table schemas
The script `generate_schemas.py` takes in input the user-defined yaml configuration files `users_and_roles.yaml` and `dbs.yaml` and calls LLaMa to generate the table schemas. The script reads from the configuration files the list of databases and the minimun number of tables to be generated along with their subject. Then, for each databases, it tries to generate the required number of tables schemas using an iterative process: sql code is created, then executed in Postgres to check if the syntax is correct. If it's not, the generate sql and the string representation of the error is given back to LLaMa, asking the LLM to fix it. This iterative process executes n times, until `--max-attempts` is reached. 
```bash
python generate_schemas.py -d ./image_configuration/user_configurations/dbs.yaml -u ./image_configuration/user_configurations/users_and_roles.yaml -m 10
```
Note: this is an optional step. If the user wants to use custom table schemas, he can do it by placing the corresponding sql or psql files inside the `./image_configuration/postgres_initialization` directory.

Note: the prompts for the LLaMa model can be found in `prompts.py`.

### Step 4: create the table rows
After the table schemas are generated, the script `generate_rows.py` must be called to generate the rows. The script takes in input the `dbs.yaml` configuration file to read the minimun number of rows to generate. The script takes in input also the directory that contains the psql scripts generated in the previous step (by convention, they start with '999_' prefix). The same iterative process of step 3 is reused here. In addition, to increase the generated rows count, the argument `-i` specifies the number of times this script will run the row generation process.

```bash
python generate_rows.py -d ./image_configuration/user_configurations/dbs.yaml -s ./image_configuration/postgres_initialization -m 8 -i 5
```

Note: the prompts for the LLaMa model can be found in `prompts.py`.


### Step 5: bulding the Docker image
The Docker image can be built with:
```bash
docker build --tag deception-db:latest
```
The Dockerfile starts from the postgres official image as base layer and:
* change the locale;
* update the system packages;
* copies all the generated psql scripts inside the container, within the directory `/docker-entrypoint-initdb.d`. When the Postgres container is started, it will execute all the scripts inside that directory. 

### Step 6: run a container
To start a container, execute:
```bash
PGDATA_ROOT=/var/lib/postgresql/data
docker run -d 
    --name deception-db
    -p 5432:5432
    -e POSTGRES_PASSWORD=password
    -e PGDATA=${PGDATA_ROOT}/pg_data
    -v $(pwd)/pg_data:${PGDATA_ROOT}
    -v "$(pwd)/image_configuration/postgres_configurations/postgres.conf":/home/postgres/postgres_conf/postgres.conf
    -v "$(pwd)/image_configuration/postgres_configurations/pg_hba.conf":/home/postgres/postgres_conf/pg_hba.conf
    deception-db:latest
    -c "config_file=/home/postgres/postgres_conf/postgres.conf"
    -c "hba_file=/home/postgres/postgres_conf/pg_hba.conf"
```
The commad above will mount the required configuration files inside the container and an optional `pg_data` directory to avoid data loss when the container is destroyed.

## A GUI for Postgres administration

```
docker pull dpage/pgadmin4
docker run --name pgadmin-container -p 5050:80 -e PGADMIN_DEFAULT_EMAIL=user@domain.com -e PGADMIN_DEFAULT_PASSWORD=pgadmin -d dpage/pgadmin4
```
* Go to localhost:5050 and login with user@domain.com and pgadmin
* Clink on add new server
* Get IP of the deception-db container: `docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' deception-db`

## Notes on the data generation process
* The 7B model is not reliable to generate correct sql syntax. 
* The prompt not only tells LLaMa what to do but also shows an example of input/output. That helps with details such as using the right namespace prefix before the table name (e.g. `pe_1_hr.Employee`)
* Even if the model is prompted to not generate any natural language, the sql code is always mixed with phrases such as "*Sure! Here are two tables for a relational database about weapons archive, inside the schema w_1_wa:*" and schema explainations. Hence the generated code is not directly executable in Postgres. To address this issue, the generated text is post-processed removing all the natural language artifacts.
* Another generation artifact are trailing commas: 
```
    CREATE TABLE IF NOT EXISTS Employee (
        employee_id SERIAL PRIMARY KEY,
        first_name VARCHAR(50) NOT NULL,
        last_name VARCHAR(50) NOT NULL,
        manager_id INT REFERENCES Employee(employee_id),
                                                    ^^^^^^ trailing comma
    );
```
The trailing commas are removed by parsing the generated output.
* In addition to trailing comma, sometimes the model forgets to place required commas. These missing commas are fixed with a python function.
* The LLM will not generate the expected number of rows. It will generate around 10 rows. Hence, the generation loop is repeated until the required number of rows is reached.
* The post-processes sql code is wrapped around a psql script and executed in Postgres to check its correctness.
* Potentially, given the simplicity of the user-defined yaml configurations for databases and users, the LLM can be used to generate also this structure.
* With the non-quantized model, the generation times are quite long. A RTX A5500 GPU with 16GB of memory can process 23/43 layers of the model, with execution times in the order of minutes for a single prompt.
```
llama_print_timings:        load time =    3615.66 ms
llama_print_timings:      sample time =     284.02 ms /  1553 runs   (    0.18 ms per token,  5467.89 tokens per second)
llama_print_timings: prompt eval time =    3615.47 ms /   450 tokens (    8.03 ms per token,   124.47 tokens per second)
llama_print_timings:        eval time =  535932.42 ms /  1552 runs   (  345.32 ms per token,     2.90 tokens per second)
llama_print_timings:       total time =  543335.51 ms
```
* With the q8 quantized model, the GPU can hold all the layers and the performance are faster:
```
llama_print_timings:        load time =     754.52 ms
llama_print_timings:      sample time =      61.15 ms /   411 runs   (    0.15 ms per token,  6721.51 tokens per second)
llama_print_timings: prompt eval time =     650.47 ms /   372 tokens (    1.75 ms per token,   571.89 tokens per second)
llama_print_timings:        eval time =   18147.71 ms /   410 runs   (   44.26 ms per token,    22.59 tokens per second)
llama_print_timings:       total time =   19328.41 ms
```


## Notes on Postgres roles, authentication, authorization and ownership

### Roles
In PostgreSQL, a role is a grouping of a specific set of capabilities, permissions, and "owned" entities. Instead of having distinct concepts of "users" and "groups", PostgreSQL uses roles to represent both of these ideas. A role can correspond to an individual person in the real world, or it can operate as a group with certain access that other roles can become members of. Roles are the anchor point within PostgreSQL that determine who authentication and authorization policies apply to.

### Authentication
Authentication is the process by which the database server establishes the identity of the client, and by extension determines whether the client application (or the user who runs the client application) is permitted to connect with the database user name that was requested. The method used to authenticate a particular client connection can be selected on the basis of (client) host address, database, and user.

In order to be used for the initial connection to the database cluster, roles must first have the LOGIN attribute set. The authentication rules themselves are defined in the host-based configuration file called pg_hba.conf. Each rule defines methods of authentication that may be scoped to the individual role. For roles that are configured for password authentication must have a password attribute set so the system can validate the supplied user password.

Each authentication record specifies a connection type, a client IP address range (if relevant for the connection type), a database name, a user name, and the authentication method to be used for connections matching these parameters. The first record with a matching connection type, client address, requested database, and user name is used to perform authentication. There is no “fall-through” or “backup”: if one record is chosen and the authentication fails, subsequent records are not considered. If no record matches, access is denied.

### Authorization
In terms of authorization, roles are defined at the database cluster level and the authorization system controls the level of access each role has to each database entity.

### Ownership
Roles are also essential to the concept of object ownership within PostgreSQL. Each database and table, for instance, have exactly one role configured as the owner. Other than superusers, the owner role is the only role that can modify or delete the actual object.
